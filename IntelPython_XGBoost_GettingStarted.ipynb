{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Copyright © 2020 Intel Corporation\n",
    "# \n",
    "# SPDX-License-Identifier: MIT\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Getting Started Example on Linear Regression\n",
    "## Importing and Organizing Data\n",
    "In this example we will be predicting prices of houses in California based on the features of each house using Intel optimizations for XGBoost shipped as a part of the oneAPI AI Analytics Toolkit.\n",
    "Let's start by **importing** all necessary data and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's **load** in the dataset and **organize** it as necessary to work with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "california = fetch_california_housing()\n",
    "\n",
    "#converting data into a pandas dataframe\n",
    "data = pd.DataFrame(california.data)\n",
    "data.columns = california.feature_names\n",
    "\n",
    "#setting price as value to be predicted, This is what to be predicted (You will get only one colomn as output and that is price\n",
    "data['PRICE'] = california.target\n",
    "\n",
    "#extracting rows, we slice. \n",
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "\n",
    "#using dmatrix values for xgboost\n",
    "#DMatrix is an internal data structure that is used by XGBoost\n",
    "#Which is optimized for both memory efficiency and training speed. \n",
    "#You can construct DMatrix from multiple different sources of data.\n",
    "#XGBoost uses CART(Classification and Regression Trees) Decision trees. \n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "#splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1693)\n",
    "#The random state hyperparameter in the train_test_split() function controls the shuffling process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate and define XGBoost regresion object** by calling the XGBRegressor() class from the library. Use hyperparameters to define the object. Intel optimizations for XGBoost trainingcan be used by calling the `hist` tree method in the parameters, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(alpha=10, base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=0.3, gamma=None,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=10, n_jobs=None, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method='hist',\n",
      "             validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Learning rate- simply means how fast the model learns\n",
    "If the learning rate is very large we will skip the optimal solution. \n",
    "If it is too small we will need too many iterations to converge to the best values.\n",
    "So using a good learning rate is crucial.\n",
    "\n",
    "Maximum depth of a tree: \n",
    "Increasing this value will make the model more complex and more likely to overfit.\n",
    "XGBoost aggressively consumes memory when training a deep tree!\n",
    "\n",
    "colsample_bytree:\n",
    "This is a family of parameters for subsampling of columns.\n",
    "All colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n",
    "colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed\n",
    "                                              \n",
    "ALPHA: L1 regularization term on weights. Increasing this value will make model more conservative. default =1 ; combats overfitting\n",
    "n_estimators — the number of runs XGBoost will try to learn; \n",
    "This is the number of trees you want to build before taking the maximum voting or averages of predictions.\n",
    "Higher number of trees give you better performance but makes your code slower\n",
    "\n",
    "tree_method string [default= auto]: The tree construction algorithm used in XGBoost\n",
    "hist: Faster histogram optimized approximate greedy algorithm.\n",
    "\n",
    "'''\n",
    "   \n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10, tree_method='hist')\n",
    "# Model without training. \n",
    "print (xg_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting and training model** using training datasets and predicting values. Note that Intel optimizations for XGBoost inference are enabled by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "8111   2.7183      48.0  3.905380   1.098330      1807.0  3.352505     33.79   \n",
      "379    2.4962      37.0  5.324324   1.108108       835.0  3.223938     37.75   \n",
      "18071  9.1569      22.0  7.252669   0.925267       773.0  2.750890     37.28   \n",
      "4462   4.9231       8.0  4.748936   1.136170       435.0  1.851064     34.10   \n",
      "6077   4.5100      22.0  5.858597   1.067873      2315.0  2.618778     34.10   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "6354   2.5275      27.0  4.246654   1.036329      1328.0  2.539197     34.14   \n",
      "14797  2.3253      14.0  4.239732   1.088852      3662.0  3.069573     32.57   \n",
      "9718   3.3472      37.0  6.625000   1.015625       266.0  4.156250     36.87   \n",
      "14789  2.3603      27.0  4.071839   1.071839       814.0  2.339080     32.58   \n",
      "1595   6.8686      35.0  6.666667   1.053030       352.0  2.666667     37.89   \n",
      "\n",
      "       Longitude  \n",
      "8111     -118.20  \n",
      "379      -122.17  \n",
      "18071    -122.01  \n",
      "4462     -118.18  \n",
      "6077     -117.85  \n",
      "...          ...  \n",
      "6354     -117.96  \n",
      "14797    -117.10  \n",
      "9718     -121.77  \n",
      "14789    -117.13  \n",
      "1595     -122.09  \n",
      "\n",
      "[5160 rows x 8 columns]\n",
      "[1.3631656 1.2941495 2.162086  ... 1.5183486 1.3439101 1.9195979]\n"
     ]
    }
   ],
   "source": [
    "xg_reg.fit(X_train,y_train) # Trained model \n",
    "# we train our model, with our data. \n",
    "preds = xg_reg.predict(X_test)\n",
    "# Trained model is to be tested with the test dataset. \n",
    "print(X_test)\n",
    "print(preds) # price here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding root mean squared error** of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0823382872176526\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE:\",rmse)\n",
    "# Accuracy for the model built is being found, We are evaluating the model. \n",
    "# Lower values of RMSE indicate better fit. \n",
    "# RMSE is a good measure of how accurately the model predicts the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Saving the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's **export the predicted values to a CSV file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds).to_csv('foo.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\n"
     ]
    }
   ],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
